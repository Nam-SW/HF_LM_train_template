MODELCFG:
    model:
        pretrained_model_name_or_path: paust/pko-t5-large
        use_cache: False
        torch_dtype: bfloat16
        device_map: balanced
        # load_in_8bit: True
    tokenizer:
        pretrained_model_name_or_path: paust/pko-t5-large
    # lora_config:
    #     inference_mode: False
    #     r: 256
    #     lora_alpha: 16
    #     lora_dropout: 0.1

TRAININGARGS:
    logging_dir: ../logs
    output_dir: ../ckpt
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 32
    num_train_epochs: 5
    learning_rate: 5e-5
    fp16: False
    bf16: True

    save_strategy: epoch
    # save_steps: 1
    save_total_limit: 10

    evaluation_strategy: epoch
    # eval_steps: 2000
    per_device_eval_batch_size: 1
    eval_accumulation_steps: 32

    optim: adamw_torch

    logging_steps: 100
    ddp_find_unused_parameters: False
    report_to: wandb
    run_name: t5_5ep

DATA:
    dataset:
        seq_len: 512

        # train_data_path: ../data/train_data/*.json
        # eval_data_path: ../data/eval_data/*.json
        train_test_split: null
        worker: 8
        batch_size: 1000
        shuffle_seed: 42

    collator:
        # mlm: False
        padding: max_length
        max_length: 512 

ETC:
    wandb_project: test
        

# remove hydra logger
hydra:
    run:
        dir: .
    job:
        chdir: false
    output_subdir: null

defaults:
    - _self_
    - override hydra/hydra_logging: disabled
    - override hydra/job_logging: disabled
